["<br/>   <br/>   <br/>      <br/>         <br/>             More Booze You Can Use<br/><br/>            When we last heard from them, the members of the<br/><br/>                  Slate<br/>                beer-testing team were coping with lagers and trying to see<br/>if they could taste the 3-to-1 price difference between the most- and<br/>least-expensive brands. (Click for a wrap-up of the first round of beer<br/>tasting.) The answer was: They found one beer they really liked, Samuel<br/>Adams Boston Lager , and one they really hated, imported Grolsch from<br/>Holland. Both were expensive beers--Grolsch was the most expensive in the<br/>test--and otherwise the testers had a hard time telling beers apart. The<br/>members of the team, as noted in the original article, all hold day jobs at<br/>Microsoft, mainly as designers, managers", ", and coders for Microsoft Word.<br/><br/>            The point of the second test was not to find the<br/>difference between cheap and expensive beers but instead to compare a variety<br/>of top-of-the-line beers. Was there one kind the tasters preferred<br/>consistently? Could they detect any of the subtleties of brewing style and<br/>provenance that microbrew customers pay such attention to when choosing some<br/>Doppelbock over a cream ale?<br/><br/>            Since the tasting panel had left the first round<br/>grumbling that cheap lagers were not a fair test of their abilities, this<br/>second round of testing was advertised to the panel as a reward. Every beer in<br/>Round 2 would be a fancy beer. A microbrew. A \"craft beer.\" A prestigious<br/>import. These were the kinds of beer the panel members said they liked--and the<br/>ones they said they were most", " familiar with. One aspect of the reward was that<br/>they would presumably enjoy the actual testing more--fewer rueful beer<br/>descriptions along the lines of \"urine\" or \"get it away!\" were expected than in<br/>the first round. The other aspect of anticipated reward was the panelists'<br/>unspoken but obvious assumption that this time they would \"do better\" on the<br/>test. Intellectual vanity being what it is, people who had fought for and won<br/>jobs at Microsoft and who still must fight every six months for primacy on the<br/>employee-ranking scale (which determines--gasp!--how many new stock options<br/>they receive) would assume that their skill as tasters was on trial, just as<br/>much as the beer was. Of course they were right, which is what made this round<br/>as amusing to administer as the first one had been.<br/><br/>            Here is what happened<br/>and", " what it meant:<br/><br/>            <br/>               1. <br/>               Procedure. This was<br/>similar in most ways to the experimental approach of Round 1. The nine testers<br/>who showed up were a subset of the original 12. The missing three dropped out<br/>with excuses of \"my wife is sick\" (one person) and \"meeting is running long\"<br/>(two).<br/><br/>            As before, each tester found before him on a table<br/>10 red plastic cups, labeled A through J. Each cup held 3 ounces of one of the<br/>beers. The A-to-J labeling scheme was the same for all testers. Instead of<br/>saltines for palate-cleansing, this time we had popcorn and nuts. As they<br/>began, the tasters were given these and only these clues:<br/><br/>            <br/><br/>               that the flight included one \"", "holdover\" beer from the previous round<br/>(Sam Adams);<br/><br/>               that it included at least one import (Bass);<br/><br/>               that it included at least one macrobrew ,<br/>specifically, a member of the vast Anheuser-Busch family (Michelob<br/>Hefeweizen).<br/><br/>            <br/><br/>            After sampling all beers, the tasters rated them as<br/>follows:<br/><br/>            <br/><br/>               <br/>                  Overall quality points, from zero to 100, reflecting their<br/>personal, subjective fondness for the beer.<br/><br/>               <br/>                  Descriptions of and comments about each<br/>beer's taste--\"smooth and nutty,\" \"too strong,\" etc. If the first ranking was a<br/>measure of how good each beer was, this was an attempt to explain what made it<br", "/>good.<br/><br/>               <br/>                  Best <br/>                  and Worst , one of each from the group.<br/><br/>               <br/>                  Name <br/>                  that beer! The tasters were told that some of<br/>the drinks were Hefeweizens, some might be IPAs (India pale ales), some might<br/>be bitters, and so on. They were asked to put each beer in its proper<br/>category--and to name a specific brewery and brand if they could. The idea here<br/>was to test the veteran beer drinkers' claim to recognize the distinctive<br/>tastes of famous brands. (To see all the grids for all the beers, click .)<br/><br/>            <br/><br/>            <br/>               2. <br/>               Philosophy. The first<br/>round of testing was All Lager. This second round was All Fancy, and Mainly Not<br/>L", "ager. As several correspondents (for instance, the of Best American<br/>Beers ) have helpfully pointed out, the definition of lager provided last<br/>time was not exactly \"accurate.\" If you want to stay within the realm of<br/>textbook definitions, a lager is a beer brewed a particular way--slowly, at<br/>cool temperatures, with yeast that settles on the bottom of the vat. This is in<br/>contrast with an ale, which is brewed faster, warmer, and with the yeast on<br/>top. By this same reasoning, lagers don't have to be light-colored,<br/>weak-flavored, and watery, as mainstream American lagers are. In principle,<br/>lagers can be dark, fierce, manly. Therefore, the correspondents suggest, it<br/>was wrong to impugn Sam Adams or Pete's Wicked for deceptive<br/>labeling, in presenting their tawnier, more", " flavorful beers as lagers too.<br/><br/>            To this the beer<br/>scientist must say: Book-learning is fine in its place. But let's be realistic.<br/>Actual drinking experience teaches the American beer consumer that a) all cheap<br/>beers are lagers; and b) most lagers are light-colored and weak. The first test<br/>was designed to evaluate low-end beers and therefore had to be lager-centric.<br/>This one is designed to test fancy beers--but in the spirit of open-mindedness<br/>and technical accuracy, it includes a few \"strong\" lagers too.<br/><br/>            <br/>               3.<br/><br/>               Materials. The 10 test beers were chosen with several goals in mind:<br/><br/>            <br/><br/>               To cover at least a modest range of fancy beer types--extra special bitter,<br/>India pale ale, Hefeweizen, and", " so on.<br/><br/>               To include both imported and domestic beers. Among the domestic microbrews,<br/>there's an obvious skew toward beers from the Pacific Northwest. But as<br/>Microsoft would put it, that's a feature not a bug. These beers all came from<br/>the Safeway nearest the Redmond, Wash., \"main campus\" of Microsoft, and<br/>microbrews are supposed to be local.<br/><br/>               To include one holdover from the previous test, as a scientific control on<br/>our tasters' preferences. This was Sam Adams , runaway winner of Round<br/>1.<br/><br/>               To include one fancy product from a monster-scale U.S. mass brewery, to see<br/>if the tasters liked it better or worse than the cute little microbrews. This<br/>was Michelob Hefeweizen , from the pride of St. Louis,<br/>Anheuser-Busch.<br", "/><br/>            <br/><br/>            Click for pricing information and pre-quaffing<br/>evaluations. The beers tasted were:<br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>             <br/><br/>            <br/>               4. Data<br/>Analysis.<br/>            <br/><br/>            <br/>               a) <br/>               Best and Worst. Compared<br/>to the lager test, we would expect the range of \"best\" choices to be more<br/>varied, since all the tested beers were supposed to be good. This expectation<br/>was most dramatically borne out in the \"Best and Worst\" rankings.<br/><br/>            The nine tasters cast a total of nine Worst", " votes<br/>and 11.5 Best votes. (Tester No. 1 turned in a sheet with three Best<br/>selections, or two more than his theoretical quota. Tester No. 4 listed a Best<br/>and a Best-minus, which counted as half a vote.)<br/><br/>             The results were clearest at the bottom: three<br/>Worsts for Pyramid Hefeweizen , even though most comments about the beer<br/>were more or less respectful. (\"Bitter, drinkable.\") But at the top and middle<br/>the situation was muddier:<br/><br/>             <br/><br/>            There were three Bests<br/>for Full Sail ESB , which most of the tasters later said they weren't<br/>familiar with, and 2.5 for Redhook IPA , which all the tasters knew. But<br/>each of these also got a Worst vote, and most of the other beers", " had a mixed<br/>reading. So far, the tasters are meeting expectations, finding something to<br/>like in nearly all these fancy beers.<br/><br/>            <br/>               b) <br/>               Overall preference<br/>points. Here the complications increase. The loser was again apparent:<br/>Pyramid Hefeweizen came in last on rating points, as it had in the<br/>Best/Worst derby. But the amazing dark horse winner was Michelob<br/>Hefeweizen . The three elements of surprise here, in ascending order of<br/>unexpectedness, are:<br/><br/>            <br/><br/>               This best-liked beer belonged to the same category, Hefeweizen, as the<br/>least-liked product, from Pyramid.<br/><br/>               This was also the only outright Anheuser-Busch product in the<br/>contest (the Redhooks are 75 percent A-B free). It is safe to", " say that all<br/>tasters would have said beforehand that they would rank an American macrobrew<br/>last, and Anheuser-Busch last of all.<br/><br/>               Although it clearly won on overall preference points, Michelob Hefeweizen<br/>was the only beer not to have received a single \"Best\" vote.<br/><br/>            <br/><br/>            The first two anomalies can be written off as<br/>testament to the power of a blind taste test. The third suggests an important<br/>difference in concepts of \"bestness.\" Sometimes a product seems to be the best<br/>of a group simply because it's the most unusual or distinctive. This is why<br/>very high Wine Spectator ratings often go to wines that mainly taste<br/>odd. But another kind of bestness involves an unobtrusive, day-in day-out<br/>acceptability. That seems to be Michelob Hefe 's achievement here: no<br", "/>one's first choice, but high on everyone's list. Let's go to the charts:<br/><br/>            This table shows how the beers performed on \"raw<br/>score\"--that is, without the advanced statistical adjustment of throwing out<br/>the highest and lowest score each beer received.<br/><br/>             <br/><br/>            Next, we have \"corrected average preference points,\"<br/>throwing out the high and low marks for each beer. The result is basically the<br/>same:<br/><br/>             <br/><br/>            It is worth noting the<br/>fate of Sam Adams on these charts. Here it ends up with a score of less<br/>than 61. These were the numbers awarded by the very same tasters who gave it a<br/>corrected preference rating of 83.33 the last time around--and 10 \"Best\" votes,<br/>vs. one Best (and one Worst) this", " time. The shift in Bests is understandable<br/>and demonstrates the importance of picking your competition. The severe drop in<br/>preference points illustrates more acutely the ancient principle of being a big<br/>fish in a small pond. These same tasters thought that Sam Adams was objectively<br/>much better when it was surrounded by Busch and Schmidt's.<br/><br/>            <br/>               c) <br/>               Value rankings. Last<br/>time this calculation led to what the colorful French would call a<br/>bouleversement. One of the cheapest beers, Busch, which had been in the<br/>lower ranks on overall preference points, came out at the top on<br/>value-for-money ratings, because it was so cheap. The big surprise now is that<br/>the highest-rated beer was also the cheapest one, Michelob Hefe ,<br/>so the value calculation turned into a rout:<br/><br/>             <br/><br", "/>            <br/>               Pyramid<br/><br/>               Hefeweizen was expensive on top of being unpopular, so its position at<br/>the bottom was hammered home--but not as painfully as that of Bass<br/>Ale . Bass had been in the respectable lower middle class of the<br/>preference rankings, so its disappointing Val-u-meter showing mainly reflects<br/>the fact that it was the only beer not on \"sale\" and therefore by far the<br/>costliest entry in the experiment.<br/><br/>            <br/>               d) <br/>               Taster skill. As members<br/>of the tasting panel began to suspect, they themselves were being judged while<br/>they judged the beer. One of the tasters, No. 7, decided to live dangerously<br/>and give specific brands and breweries for Samples A through J. This man was<br/>the only panel member whose job does not involve designing Microsoft Word--and<br/>the only one", " to identify two or more of the beers accurately and specifically.<br/>(He spotted Redhook IPA and Redhook ESB.) The fact that the beers correctly<br/>identified were the two most popular microbrews in the Seattle area suggests<br/>that familiarity is the main ingredient in knowing your beer.<br/><br/>            Many others were simply lost. Barely half the<br/>tasters, five of nine, recognized that Michelob Hefeweizen <br/>               was a<br/>Hefeweizen. Before the test, nine of nine would have said that picking out a<br/>Hefe was easy, because of its cloudy look and wheaty flavor. Three tasters<br/>thought Sam Adams was an IPA ; two thought Redhook's IPA was a<br/>Hefeweizen. In fairness, six of nine testers identified Pyramid<br/>Hefeweizen as a Hefe, and six recognized Full Sail ESB as a bitter.<br/>Much in the", " fashion of blind men describing an elephant, here is a how the<br/>testers handled Sam Adams Boston Lager :<br/><br/>             <br/><br/>            <br/>               5.<br/>Implications <br/>               and Directions for Future Research. Science does<br/>not always answer questions; often, it raises many new ones. This excursion<br/>into beer science mainly raises the question: What kind of people are we?<br/><br/>            If we are Gradgrind-like empiricists, living our<br/>life for \"welfare maximization\" as described in introductory econ. courses, the<br/>conclusion is obvious. We learned from the first experiment to buy<br/>either Sam Adams (when we wanted maximum lager enjoyment per bottle)<br/>or Busch (for maximum taste and snob appeal per dollar). From this<br/>second round we see an even more efficient possibility: Buy Michelob<br/>Hefeweizen and", " nothing else, since on the basis of this test it's the best<br/>liked and the cheapest beer. By the way, if there is a single company<br/>whose achievements the testing panel honored, it would be<br/>Anheuser-Busch . From its brewing tanks came two of the double-crown<br/>winners of the taste tests: plain old Busch , the Taste-o-meter<br/>and Snob-o-meter victor of Round 1, and Michelob Hefeweizen , the<br/>preference-point and Val-u-meter winner this time.<br/><br/>            But, of course, there is another possibility: that<br/>what is excluded in a blind taste test is in fact what we want, and are happy<br/>to pay for, when we sit down with a beer. The complicated label, the fancy<br/>bottle, the exotic concept that this beer has traveled from some far-off corner<br/>of Bohemia or even the Yak", "ima Valley--all this may be cheap at the<br/>$1.25-per-pint cost difference between the cheapest and the most expensive<br/>beers. In elementary school, we all endured a standard science experiment: If<br/>you shut your eyes and pinch your nose closed, can you tell any difference in<br/>the taste of a slice of apple, of carrot, of pear? You can't--but that doesn't<br/>mean that from then on you should close your eyes, hold your nose, and chew a<br/>cheap carrot when you feel like having some fruit. There is a time and place<br/>for carrots, but also for juicy pears. There is a time for Busch, but also for<br/>Full Sail \"Equinox.\"<br/><br/>            For scientists who want to continue this work at<br/>home, here are a few suggestions for further research:<br/><br/>            <br/><br/>               Tell the testers ahead of", " time what beers they will be drinking. Ask them<br/>to rank the beers, 1 through 10, based on how well they like them. Then compare<br/>the list with the \"revealed preferences\" that come from the blind test.<br/><br/>               As a variation, show them the list ahead of time and ask them to pick out<br/>the beer they know they love and the one they know they hate. Then compare this<br/>with the \"after\" list.<br/><br/>               If you're going to test imported lagers, try Foster's or Corona rather than<br/>Grolsch.<br/><br/>               Remember to stay strictly in the scientist's role. Don't take the test<br/>yourself.<br/><br/>            <br/><br/>         <br/>      <br/>   <br/>"]